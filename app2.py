import gradio as gr
import requests
import json
import random
from typing import Any, Dict, List, Optional


DEFAULT_OLLAMA_URL = "http://localhost:11434"


def check_ollama_connection(ollama_url: str) -> bool:
    try:
        r = requests.get(f"{ollama_url}/api/tags", timeout=5)
        r.raise_for_status()
        return True
    except requests.RequestException:
        return False


def get_ollama_models(ollama_url: str) -> List[str]:
    try:
        r = requests.get(f"{ollama_url}/api/tags", timeout=5)
        r.raise_for_status()
        data = r.json()
        return [m.get("name", "") for m in data.get("models", [])]
    except requests.RequestException:
        return []


def pick_default_model(models: List[str]) -> str:
    for cand in ["exaone3.5", "exaone:latest", "exaone"]:
        if cand in models:
            return cand
    for m in models:
        if "exaone" in m.lower():
            return m
    return models[0] if models else ""


def generate_once(
    ollama_url: str,
    model: str,
    prompt: str,
    options: Optional[Dict[str, Any]] = None,
    timeout_s: Any = (3, 15),
) -> Optional[str]:
    merged_opts: Dict[str, Any] = {"num_predict": 256}
    if options:
        try:
            merged_opts.update({k: v for k, v in options.items() if v is not None})
        except Exception:
            pass
    payload = {"model": model, "prompt": prompt, "stream": False, "options": merged_opts}
    try:
        r = requests.post(f"{ollama_url}/api/generate", json=payload, timeout=timeout_s)
        r.raise_for_status()
        data = r.json()
        return data.get("response", "")
    except requests.RequestException:
        return None

def stream_generate(
    ollama_url: str,
    model: str,
    prompt: str,
    options: Optional[Dict[str, Any]] = None,
    # Short connect/read timeouts to improve cancel responsiveness
    timeout_s: Any = (3, 5),
):
    # Merge conservative defaults to avoid very long generations
    merged_opts: Dict[str, Any] = {"num_predict": 256}
    if options:
        try:
            merged_opts.update({k: v for k, v in options.items() if v is not None})
        except Exception:
            pass
    payload = {"model": model, "prompt": prompt, "options": merged_opts}
    try:
        with requests.post(
            f"{ollama_url}/api/generate",
            json=payload,
            stream=True,
            # Accept tuple timeouts (connect, read) for better responsiveness
            timeout=timeout_s,
        ) as resp:
            resp.raise_for_status()
            for line in resp.iter_lines():
                if not line:
                    continue
                try:
                    j = json.loads(line.decode("utf-8"))
                except json.JSONDecodeError:
                    continue
                if "response" in j:
                    yield j["response"]
                if j.get("done"):
                    break
    except requests.RequestException:
        return


def build_agent_prompt(
    role_title: str,
    stance_instruction: str,
    system_rules: str,
    user_question: str,
    memory_text: str,
    depth: int,
    extra_directive: str = "",
    ) -> str:
    return (
        "í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n"
        f"ë‹¹ì‹ ì€ '{role_title}' ì…ë‹ˆë‹¤. {stance_instruction}\n\n"
        f"ê·œì¹™:\n{system_rules}\n\n"
        f"ì§ˆë¬¸:\n{user_question}\n\n"
        f"ëŒ€í™” ë©”ëª¨(ìš”ì•½):\n{memory_text}\n\n"
        f"ìš”ì²­:\n- ì§ì „ ìƒëŒ€ ë°œì–¸ì„ í•œ ì¤„ë¡œ ìš”ì•½\n- í•µì‹¬ ì£¼ì¥ì„ 2ë¬¸ì¥ ì´ë‚´ë¡œ ëª…í™•íˆ\n- í•„ìš”í•  ë•Œë§Œ ì½”ë“œ ë¸”ë¡ í¬í•¨(ì—†ì–´ë„ ë¨)\n- ê³µì†í•˜ì§€ë§Œ ë‹¨í˜¸í•˜ê²Œ ìƒí˜¸ì‘ìš©ì ìœ¼ë¡œ ì‘ë‹µ\n- ê¹Šì´ ë ˆë²¨: {depth}\n{extra_directive}\n"
    )


def build_agent_prompt_natural(
    role_title: str,
    stance_instruction: str,
    system_rules: str,
    user_question: str,
    memory_text: str,
    depth: int,
    extra_directive: str = "",
) -> str:
    return (
        "í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  íšŒí™”ì²´ë¡œ ë‹µí•˜ì„¸ìš”.\n"
        f"ë‹¹ì‹ ì€ '{role_title}'ì…ë‹ˆë‹¤. {stance_instruction}\n"
        f"ì•„ë˜ ê·œì¹™ì€ ì¡°ìš©íˆ ì°¸ê³ ë§Œ í•˜ê³  ì¶œë ¥ì—ëŠ” ë“œëŸ¬ë‚´ì§€ ë§ˆì„¸ìš”.\n"
        f"ê·œì¹™: {system_rules}\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸: {user_question}\n"
        f"ìµœê·¼ ëŒ€í™” ìš”ì•½: {memory_text}\n"
        f"ëŒ€í™” ì‹¬ë„: {depth}\n"
        f"ì¶œë ¥ ì§€ì¹¨: ì„¹ì…˜ ì œëª©ì´ë‚˜ 'ìš”ì•½:', 'ì£¼ì¥:' ê°™ì€ ë ˆì´ë¸”ì„ ì“°ì§€ ë§ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ëŒ€í™”í•˜ì„¸ìš”. {extra_directive}\n"
    )


def trim_memory(convo: List[Dict[str, str]], limit_chars: int = 2000) -> str:
    text = ""
    total = 0
    for msg in reversed(convo):
        line = f"{msg.get('name', msg.get('role',''))}: {msg.get('content','')}\n"
        total += len(line)
        if total > limit_chars:
            break
        text = line + text
    return text.strip()


def choose_fixed_stance(user_question: str) -> str:
    q = (user_question or "").lower()
    mapping = [
        ("greedy", "ê·¸ë¦¬ë””"),
        ("ê·¸ë¦¬ë””", "ê·¸ë¦¬ë””"),
        ("dynamic", "ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°"),
        ("ë‹¤ì´ë‚˜ë¯¹", "ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°"),
        ("ë™ì ", "ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°"),
        ("dp", "ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°"),
        ("divide", "ë¶„í•  ì •ë³µ"),
        ("ë¶„í• ", "ë¶„í•  ì •ë³µ"),
        ("backtracking", "ë°±íŠ¸ë˜í‚¹"),
        ("ë°±íŠ¸", "ë°±íŠ¸ë˜í‚¹"),
        ("bfs", "íƒìƒ‰(BFS/DFS)"),
        ("dfs", "íƒìƒ‰(BFS/DFS)"),
        ("íƒìƒ‰", "íƒìƒ‰(BFS/DFS)"),
        ("hash", "í•´ì‹œ ê¸°ë°˜"),
        ("í•´ì‹œ", "í•´ì‹œ ê¸°ë°˜"),
    ]
    for key, stance in mapping:
        if key in q:
            return stance
    candidates = [
        "ê·¸ë¦¬ë””",
        "ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°",
        "ë¶„í•  ì •ë³µ",
        "ë°±íŠ¸ë˜í‚¹",
        "íƒìƒ‰(BFS/DFS)",
        "í•´ì‹œ ê¸°ë°˜",
    ]
    return random.choice(candidates)


def main():
    with gr.Blocks(css='''
        .gradio-container{background:linear-gradient(180deg,#fdf2f8 0%,#eef2ff 100%)}
        .btn-wide button{width:100%; border-radius:12px; padding:10px 12px; background:linear-gradient(90deg,#6366f1,#22d3ee); color:#fff; border:none}
        .btn-wide button:hover{filter:brightness(1.06)}
        .panel{padding:14px;border-radius:14px;border:1px solid #e5e7eb;box-shadow:0 6px 20px rgba(0,0,0,.06)}
        .panel h3{margin-top:0}
        .panel textarea, .panel input{border-radius:10px !important; border:1px solid #e5e7eb !important}
        .chatbox{background:#fff;border:1px solid #e5e7eb;border-radius:14px;box-shadow:0 4px 14px rgba(0,0,0,.06)}
    ''') as demo:
        gr.Markdown("# AI í† ë¡ ì¥ Â· AI Debate Studio")
        models0 = get_ollama_models(DEFAULT_OLLAMA_URL)
        default_model = pick_default_model(models0)

        with gr.Row():
            ollama_url = gr.Textbox(label="Ollama URL", value=DEFAULT_OLLAMA_URL)
            model_dd = gr.Dropdown(label="Model", choices=models0, value=default_model)
            fast_mode = gr.Checkbox(label="Fast mode (no streaming)", value=True)
            start_btn = gr.Button("Start")
            stop_btn = gr.Button("Stop", interactive=False)
            reset_btn = gr.Button("Reset")
            status = gr.Textbox(label="Status", interactive=False)

        # Left stacked panels; right topic+chat
        with gr.Row():
            with gr.Column(scale=1, elem_classes=["panel"]):
                gr.Markdown("### ğŸŸ£ Greedy Rebel ì„¤ì •")
                agent1_name = gr.Textbox(label="ì´ë¦„", value="Greedy Rebel")
                agent1_system = gr.TextArea(
                    label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                    value=("ì£¼ì œì— ë§ì¶° í•œ ê°€ì§€ ì…ì¥ì„ ìŠ¤ìŠ¤ë¡œ ì •í•˜ê³ ,\n"
                           "í† ë¡  ë‚´ë‚´ ê·¸ ì…ì¥ë§Œ ê³ ìˆ˜í•˜ë¼(ì „í™˜ ê¸ˆì§€).\n"
                           "í•„ìš”í•  ë•Œë§Œ ê°„ê²°í•œ ì½”ë“œë¡œ ì„¤ë“í•˜ë¼."),
                    lines=5,
                )
                agent1_mem = gr.Slider(label="Memory Size (chars)", minimum=200, maximum=10000, value=2000, step=100)

                gr.Markdown("### ğŸ”µ DP Master ì„¤ì •")
                agent2_name = gr.Textbox(label="ì´ë¦„", value="DP Master")
                agent2_system = gr.TextArea(
                    label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                    value=("ì£¼ì œì— ëŒ€í•´ êµ¬ì¡°ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ê´€ì ì„ ì œì‹œí•˜ë¼.\n"
                           "íŠ¹ì • ê¸°ë²•ì— ì§‘ì°©í•˜ì§€ ë§ê³ , ìƒí™©ì— ë§ê²Œ ë°˜ëŒ€ ì…ì¥ì„ ì·¨í•˜ë¼."),
                    lines=5,
                )
                agent2_mem = gr.Slider(label="Memory Size (chars)", minimum=200, maximum=10000, value=2000, step=100)

            with gr.Column(scale=2, elem_classes=["panel"]):
                user_question = gr.Textbox(label="ğŸ“ í† ë¡  ì£¼ì œ", value="ê·¸ë¦¬ë””ì™€ ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°, ì–´ëŠ ìª½ì´ ë” ì¢‹ì„ê¹Œ?")
                chat = gr.Chatbot(label="ğŸ—¨ï¸ ì¸í„°ë™í‹°ë¸Œ í† ë¡ ", height=640, type="messages", elem_classes=["chatbox"])
                with gr.Row(equal_height=True):
                    btn_style = {"elem_classes": ["btn-wide"]}
                    opt_1 = gr.Button("1ë²ˆ ì˜ê²¬ ì‹¬í™”", visible=False, **btn_style)
                    opt_2 = gr.Button("2ë²ˆ ì˜ê²¬ ì‹¬í™”", visible=False, **btn_style)
                    opt_3 = gr.Button("ê³„ì† í† ë¡ ", visible=False, **btn_style)
                    opt_4 = gr.Button("ì¶”ê°€ ì§ˆë¬¸", visible=False, **btn_style)
                    opt_5 = gr.Button("ìƒˆë¡œìš´ ì§ˆë¬¸", visible=False, **btn_style)

                followup_tb = gr.Textbox(label="ì¶”ê°€ ì§ˆë¬¸ ì…ë ¥", placeholder="ì—¬ê¸°ì— ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", visible=False)
                followup_send = gr.Button("ë³´ë‚´ê¸°", visible=False)
        # Hidden rules default
        system_rules = gr.State("- ì„œë¡œ ë°˜ëŒ€ ì…ì¥ì„ ìœ ì§€\n- í•µì‹¬ë§Œ ê°„ê²°íˆ, í•„ìš”í•  ë•Œë§Œ ì½”ë“œ\n- ë¹„ë°©/ëª¨ìš• ê¸ˆì§€, ê±´ì„¤ì  í† ë¡ \n- ì‚¬ìš©ìê°€ ì„ íƒí•œ íë¦„ì„ ë”°ë¥´ê¸°")

        state = gr.State({
            "running": False,
            "history": [],
            "convo": [],
            "depth": 1,
            "last_choice": None,
            "agent_index": None,
            "agent1_fixed_stance": None,
            "agents": [
                {"name": "Greedy Rebel", "role_title": "Greedy Rebel", "system": "", "temperature": 0.9, "top_k": 40, "memory_size": 2000},
                {"name": "DP Master", "role_title": "DP Master", "system": "", "temperature": 0.9, "top_k": 40, "memory_size": 2000},
            ],
        })

        def on_url_change(url: str):
            models = get_ollama_models(url)
            return gr.Dropdown.update(choices=models, value=pick_default_model(models))

        ollama_url.change(on_url_change, inputs=[ollama_url], outputs=[model_dd])

        def on_start(q: str, rules: str, url: str, model: str, fast: bool,
                     a1_name: str, a1_sys: str, a1_mem: int,
                     a2_name: str, a2_sys: str, a2_mem: int,
                     st: Dict[str, Any]):
            if st.get("running"):
                opts = [gr.update(visible=True)] * 5
                return (st["history"], gr.update(interactive=False), gr.update(interactive=True), *opts, "ì´ë¯¸ ì§„í–‰ ì¤‘")

            if not check_ollama_connection(url):
                opts = [gr.update(visible=False)] * 5
                return (st["history"], gr.update(interactive=True), gr.update(interactive=False), *opts, "Ollama ì—°ê²° ì‹¤íŒ¨")

            st["running"] = True
            st["history"] = []
            st["convo"] = []
            st["depth"] = 1
            st["last_choice"] = None
            st["agent_index"] = random.randint(0, 1)

            st["agents"][0].update({
                "name": (a1_name or "Greedy Rebel").strip(),
                "system": a1_sys.strip(),
                "memory_size": int(a1_mem),
            })
            st["agents"][1].update({
                "name": (a2_name or "DP Master").strip(),
                "system": a2_sys.strip(),
                "memory_size": int(a2_mem),
            })

            st["agent1_fixed_stance"] = choose_fixed_stance(q)

            mem = trim_memory(st["convo"], 1600)
            a1 = st["agents"][0]
            fixed = st.get("agent1_fixed_stance", "ê·¸ë¦¬ë””")
            p1 = build_agent_prompt_natural(
                a1["role_title"], f"{a1['system']}\nì´ë²ˆ í† ë¡ ì˜ ê³ ì • ì…ì¥: '{fixed}'. ì´ ì…ì¥ë§Œ ìœ ì§€í•˜ì—¬ ë‹µí•˜ë¼.",
                rules, q, mem, st["depth"],
                extra_directive=f"'{fixed}' ê´€ì ê³¼ ì¼ê´€ëœ ê²½ìš°ì—ë§Œ ì½”ë“œ í¬í•¨(í•„ìš” ì‹œ).",
            )

            a2 = st["agents"][1]
            p2 = build_agent_prompt_natural(
                a2["role_title"], f"{a2['system']}\nì§ì „ ë°œì–¸ì— ëŒ€í•´ ìƒí˜¸ì‘ìš©ì ìœ¼ë¡œ ë°˜ëŒ€ ì…ì¥ì„ ì œì‹œí•˜ë¼.",
                rules, q, mem, st["depth"],
            )

            start_dis = gr.update(interactive=False)
            stop_en = gr.update(interactive=True)
            opts_hidden = [gr.update(visible=False)] * 5
            status_text = f"í† ë¡  ì‹œì‘ Â· ê³ ì • ì…ì¥: {fixed}"

            # Agent 1
            full1 = ""
            st["history"].append({"role": "user", "content": f"{a1['name']}: "})
            st["convo"].append({"name": a1["name"], "content": ""})
            yield (st["history"], start_dis, stop_en, *opts_hidden, status_text)
            try:
                if fast:
                    _res1 = generate_once(url, model, p1, options={"temperature": a1.get("temperature", 0.9), "top_k": a1.get("top_k", 40)})
                    chunk_iter = [(_res1 or "")]
                else:
                    chunk_iter = stream_generate(url, model, p1, options={"temperature": a1.get("temperature", 0.9), "top_k": a1.get("top_k", 40)})
                for chunk in chunk_iter:
                    if not st.get("running"):
                        break
                    full1 += chunk
                    _d1 = clean_labels(full1) if 'clean_labels' in globals() else full1
                    st["history"][-1]["content"] = f"{a1['name']}: {_d1}"
                    st["convo"][-1]["content"] = _d1
                    yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} íƒ€ì´í•‘ ì¤‘...")
            except Exception:
                st["history"][-1]["content"] = f"{a1['name']}: (ìƒì„± ì¤‘ ì˜¤ë¥˜)"
                st["convo"][-1]["content"] = "(ìƒì„± ì¤‘ ì˜¤ë¥˜)"

            # Agent 2
            full2 = ""
            st["history"].append({"role": "assistant", "content": f"{a2['name']}: "})
            st["convo"].append({"name": a2["name"], "content": ""})
            yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} íƒ€ì´í•‘ ì¤‘...")
            try:
                if fast:
                    _res2 = generate_once(url, model, p2, options={"temperature": a2.get("temperature", 0.9), "top_k": a2.get("top_k", 40)})
                    chunk_iter2 = [(_res2 or "")]
                else:
                    chunk_iter2 = stream_generate(url, model, p2, options={"temperature": a2.get("temperature", 0.9), "top_k": a2.get("top_k", 40)})
                for chunk in chunk_iter2:
                    if not st.get("running"):
                        break
                    full2 += chunk
                    _d2 = clean_labels(full2) if 'clean_labels' in globals() else full2
                    st["history"][-1]["content"] = f"{a2['name']}: {_d2}"
                    st["convo"][-1]["content"] = _d2
                    yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} íƒ€ì´í•‘ ì¤‘...")
            except Exception:
                st["history"][-1]["content"] = f"{a2['name']}: (ìƒì„± ì¤‘ ì˜¤ë¥˜)"
                st["convo"][-1]["content"] = "(ìƒì„± ì¤‘ ì˜¤ë¥˜)"

            yield (st["history"], start_dis, stop_en, *[gr.update(visible=True)]*5, "ë‹¤ìŒ ì§„í–‰ì„ ì„ íƒí•˜ì„¸ìš”")

        def on_stop(st: Dict[str, Any]):
            st["running"] = False
            return (st["history"], gr.update(interactive=True), gr.update(interactive=False), *[gr.update(visible=True)]*5, "ì •ì§€ë¨")

        def on_reset(st: Dict[str, Any]):
            st.update({"running": False, "history": [], "convo": [], "depth": 1, "last_choice": None, "agent_index": None})
            return ([], gr.update(interactive=True), gr.update(interactive=False), *[gr.update(visible=False)]*5, "ì´ˆê¸°í™” ì™„ë£Œ")

        def proceed(choice: str, q: str, rules: str, url: str, model: str, st: Dict[str, Any]):
            if not st.get("running"):
                st["running"] = True
            st["last_choice"] = choice
            st["depth"] = max(1, st.get("depth", 1)) + 1

            a1 = st["agents"][0]
            a2 = st["agents"][1]
            mem = trim_memory(st["convo"], 2000)

            # Quick connectivity check to avoid long hangs
            if not check_ollama_connection(url):
                return (
                    st["history"],
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    *[gr.update(visible=True)] * 5,
                    "Ollama ì—°ê²° ì‹¤íŒ¨ (ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”)",
                )

            if choice == "opt4":
                # Follow-up question within current context: use q as user message
                st["history"].append({"role": "user", "content": f"User: {q}"})
                st["convo"].append({"name": "User", "content": q})
                mem2 = trim_memory(st["convo"], 2000)
                p1 = build_agent_prompt(a1["role_title"], a1["system"], rules, q, mem2, st["depth"],)
                p2 = build_agent_prompt(a2["role_title"], a2["system"], rules, q, mem2, st["depth"],)

                start_dis = gr.update(interactive=False)
                stop_en = gr.update(interactive=True)
                opts_hidden = [gr.update(visible=False)] * 5

                # Agent 1 reply
                full1 = ""
                st["history"].append({"role": "user", "content": f"{a1['name']}: "})
                st["convo"].append({"name": a1["name"], "content": ""})
                yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} íƒ€ì´í•‘ ì¤‘...")
                try:
                    for chunk in stream_generate(url, model, p1, options={"temperature": a1.get("temperature", 0.9), "top_k": a1.get("top_k", 40)}):
                        if not st.get("running"):
                            break
                        full1 += chunk
                        st["history"][-1]["content"] = f"{a1['name']}: {full1}"
                        st["convo"][-1]["content"] = full1
                        yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} íƒ€ì´í•‘ ì¤‘...")
                except Exception:
                    st["history"][-1]["content"] = f"{a1['name']}: (ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"
                    st["convo"][-1]["content"] = "(ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"

                # Agent 2 reply
                full2 = ""
                st["history"].append({"role": "assistant", "content": f"{a2['name']}: "})
                st["convo"].append({"name": a2["name"], "content": ""})
                yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} íƒ€ì´í•‘ ì¤‘...")
                try:
                    for chunk in stream_generate(url, model, p2, options={"temperature": a2.get("temperature", 0.9), "top_k": a2.get("top_k", 40)}):
                        if not st.get("running"):
                            break
                        full2 += chunk
                        st["history"][-1]["content"] = f"{a2['name']}: {full2}"
                        st["convo"][-1]["content"] = full2
                        yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} íƒ€ì´í•‘ ì¤‘...")
                except Exception:
                    st["history"][-1]["content"] = f"{a2['name']}: (ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"
                    st["convo"][-1]["content"] = "(ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"

                yield (st["history"], start_dis, stop_en, *[gr.update(visible=True)]*5, "ë‹¤ìŒ ì§„í–‰ì„ ì„ íƒí•˜ì„¸ìš”")
                return

            if choice == "opt1":
                fixed = st.get("agent1_fixed_stance", "ê³ ì • ì…ì¥")
                directive = f"ì§ì „ ë°œì–¸ì— ëŒ€í•´ '{fixed}' ê´€ì ìœ¼ë¡œ ë°˜ë°•í•˜ê³ , í•„ìš”í•  ë•Œë§Œ ì½”ë“œ í¬í•¨"
                prompt = build_agent_prompt(a1["role_title"], f"{a1['system']}\ní•­ìƒ '{fixed}' ì…ì¥ë§Œ ìœ ì§€.", rules, q, mem, st["depth"], directive)
                speaker = a1
            elif choice == "opt2":
                directive = "ì§ì „ ë°œì–¸ì— ëŒ€í•´ ë°˜ëŒ€ ì…ì¥ì„ ìƒí˜¸ì‘ìš©ì ìœ¼ë¡œ ì œì‹œí•˜ê³ , í•„ìš”í•  ë•Œë§Œ ì½”ë“œ í¬í•¨"
                prompt = build_agent_prompt(a2["role_title"], a2["system"], rules, q, mem, st["depth"], directive)
                speaker = a2
            elif choice == "opt3":
                fixed = st.get("agent1_fixed_stance", "ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½")
                p1 = build_agent_prompt(
                    a1["role_title"], f"{a1['system']}\nï¿½×»ï¿½ '{fixed}' ï¿½ï¿½ï¿½å¸¸ ï¿½ï¿½ï¿½ï¿½.",
                    rules, q, mem, st["depth"],
                    extra_directive=f"'{fixed}' ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½Ï°ï¿½ï¿½ï¿½ ï¿½ï¿½ì¿¡ï¿½ï¿½ ï¿½Úµï¿½ ï¿½ï¿½ï¿½ï¿½(ï¿½Ê¿ï¿½ ï¿½ï¿½).",
                )
                p2 = build_agent_prompt(
                    a2["role_title"], a2["system"],
                    rules, q, mem, st["depth"],
                    extra_directive="ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¡œ, 'ìš”ì•½:' 'ì£¼ì¥:' ê°™ì€ ë ˆì´ë¸”ì€ ì“°ì§€ ë§ˆì„¸ìš”.",
                )

                start_dis = gr.update(interactive=False)
                stop_en = gr.update(interactive=True)
                opts_hidden = [gr.update(visible=False)] * 5

                # Agent 1
                full1 = ""
                st["history"].append({"role": "user", "content": f"{a1['name']}: "})
                st["convo"].append({"name": a1["name"], "content": ""})
                yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} Å¸ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½...")
                try:
                    for chunk in stream_generate(url, model, p1, options={"temperature": a1.get("temperature", 0.9), "top_k": a1.get("top_k", 40)}):
                        if not st.get("running"):
                            break
                        full1 += chunk
                        _disp1 = full1
                        try:
                            _disp1 = clean_labels(full1)
                        except Exception:
                            pass
                        st["history"][-1]["content"] = f"{a1['name']}: {_disp1}"
                        st["convo"][-1]["content"] = _disp1
                        yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} Å¸ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½...")
                except Exception:
                    st["history"][-1]["content"] = f"{a1['name']}: (ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½)"
                    st["convo"][-1]["content"] = "(ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½)"

                # Agent 2
                full2 = ""
                st["history"].append({"role": "assistant", "content": f"{a2['name']}: "})
                st["convo"].append({"name": a2["name"], "content": ""})
                yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} Å¸ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½...")
                try:
                    for chunk in stream_generate(url, model, p2, options={"temperature": a2.get("temperature", 0.9), "top_k": a2.get("top_k", 40)}):
                        if not st.get("running"):
                            break
                        full2 += chunk
                        _disp2 = full2
                        try:
                            _disp2 = clean_labels(full2)
                        except Exception:
                            pass
                        st["history"][-1]["content"] = f"{a2['name']}: {_disp2}"
                        st["convo"][-1]["content"] = _disp2
                        yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} Å¸ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½...")
                except Exception:
                    st["history"][-1]["content"] = f"{a2['name']}: (ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½)"
                    st["convo"][-1]["content"] = "(ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½)"

                yield (st["history"], start_dis, stop_en, *[gr.update(visible=True)]*5, "ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½Ï¼ï¿½ï¿½ï¿½")
                return
            else:
                st["history"].append({"role": "user", "content": "ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ì‹œì‘ì„ ëˆ„ë¥´ì„¸ìš”."})
                return (st["history"], gr.update(interactive=True), gr.update(interactive=True), *[gr.update(visible=False)]*5, "ì…ë ¥ ëŒ€ê¸°")

            full = ""
            role = "user" if speaker["name"] == a1["name"] else "assistant"
            st["history"].append({"role": role, "content": f"{speaker['name']}: "})
            st["convo"].append({"name": speaker["name"], "content": ""})
            opts_hidden = [gr.update(visible=False)]*5
            # Immediate UI feedback before network call
            yield (
                st["history"],
                gr.update(interactive=False),
                gr.update(interactive=True),
                *opts_hidden,
                f"{speaker['name']} ì¤€ë¹„ ì¤‘...",
            )
            try:
                for chunk in stream_generate(url, model, prompt, options={"temperature": speaker.get("temperature", 0.95), "top_k": speaker.get("top_k", 40)}):
                    if not st.get("running"):
                        break
                    full += chunk
                    st["history"][-1]["content"] = f"{speaker['name']}: {full}"
                    st["convo"][-1]["content"] = full
                    yield (st["history"], gr.update(interactive=False), gr.update(interactive=True), *opts_hidden, f"{speaker['name']} íƒ€ì´í•‘ ì¤‘...")
            except Exception:
                st["history"][-1]["content"] = f"{speaker['name']}: (ìƒì„± ì¤‘ ì˜¤ë¥˜)"
                st["convo"][-1]["content"] = "(ìƒì„± ì¤‘ ì˜¤ë¥˜)"

            yield (st["history"], gr.update(interactive=False), gr.update(interactive=True), *[gr.update(visible=True)]*5, "ë‹¤ìŒ ì§„í–‰ì„ ì„ íƒí•˜ì„¸ìš”")

        start_btn.click(
            on_start,
            inputs=[
                user_question, system_rules, ollama_url, model_dd, fast_mode,
                agent1_name, agent1_system, agent1_mem,
                agent2_name, agent2_system, agent2_mem,
                state,
            ],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
            queue=True,
        )

        stop_btn.click(
            on_stop,
            inputs=[state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
        )

        reset_btn.click(
            on_reset,
            inputs=[state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
        )

        opt_1.click(
            proceed,
            inputs=[gr.State("opt1"), user_question, system_rules, ollama_url, model_dd, state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
            queue=True,
        )
        opt_2.click(
            proceed,
            inputs=[gr.State("opt2"), user_question, system_rules, ollama_url, model_dd, state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
            queue=True,
        )
        opt_3.click(
            proceed,
            inputs=[gr.State("opt3"), user_question, system_rules, ollama_url, model_dd, state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
            queue=True,
        )
        def show_followup(st: Dict[str, Any]):
            return (
                st.get("history", []),
                gr.update(interactive=True),
                gr.update(interactive=True),
                gr.update(visible=True), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True),
                "ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ë³´ë‚´ê¸°ë¥¼ ëˆ„ë¥´ì„¸ìš”",
                gr.update(value="", visible=True),
                gr.update(visible=True),
            )

        opt_4.click(
            show_followup,
            inputs=[state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status, followup_tb, followup_send],
        )

        def on_send_followup(subq: str, url: str, model: str, st: Dict[str, Any]):
            a1 = st["agents"][0]
            a2 = st["agents"][1]
            if not subq.strip():
                return (
                    st.get("history", []), gr.update(interactive=True), gr.update(interactive=True),
                    gr.update(visible=True), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True),
                    "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", gr.update(visible=True), gr.update(visible=True),
                )
            st.setdefault("convo", [])
            st.setdefault("history", [])
            st["history"].append({"role": "user", "content": f"User: {subq}"})
            st["convo"].append({"name": "User", "content": subq})
            mem2 = trim_memory(st["convo"], 2000)

            p1 = build_agent_prompt(a1["role_title"], a1["system"], system_rules.value if hasattr(system_rules, 'value') else "", subq, mem2, st.get("depth", 1))
            p2 = build_agent_prompt(a2["role_title"], a2["system"], system_rules.value if hasattr(system_rules, 'value') else "", subq, mem2, st.get("depth", 1))

            start_dis = gr.update(interactive=False)
            stop_en = gr.update(interactive=True)
            opts_hidden = [gr.update(visible=False)] * 5

            full1 = ""
            st["history"].append({"role": "user", "content": f"{a1['name']}: "})
            st["convo"].append({"name": a1["name"], "content": ""})
            yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} íƒ€ì´í•‘ ì¤‘...", gr.update(visible=True), gr.update(visible=True))
            try:
                for chunk in stream_generate(url, model, p1, options={"temperature": a1.get("temperature", 0.9), "top_k": a1.get("top_k", 40)}):
                    full1 += chunk
                    st["history"][-1]["content"] = f"{a1['name']}: {full1}"
                    st["convo"][-1]["content"] = full1
                    yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a1['name']} íƒ€ì´í•‘ ì¤‘...", gr.update(visible=True), gr.update(visible=True))
            except Exception:
                st["history"][-1]["content"] = f"{a1['name']}: (ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"
                st["convo"][-1]["content"] = "(ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"

            full2 = ""
            st["history"].append({"role": "assistant", "content": f"{a2['name']}: "})
            st["convo"].append({"name": a2["name"], "content": ""})
            yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} íƒ€ì´í•‘ ì¤‘...", gr.update(visible=True), gr.update(visible=True))
            try:
                for chunk in stream_generate(url, model, p2, options={"temperature": a2.get("temperature", 0.9), "top_k": a2.get("top_k", 40)}):
                    full2 += chunk
                    st["history"][-1]["content"] = f"{a2['name']}: {full2}"
                    st["convo"][-1]["content"] = full2
                    yield (st["history"], start_dis, stop_en, *opts_hidden, f"{a2['name']} íƒ€ì´í•‘ ì¤‘...", gr.update(visible=True), gr.update(visible=True))
            except Exception:
                st["history"][-1]["content"] = f"{a2['name']}: (ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"
                st["convo"][-1]["content"] = "(ì‘ë‹µ ì¤‘ ì˜¤ë¥˜)"

            yield (
                st["history"], start_dis, stop_en,
                *[gr.update(visible=True)]*5,
                "ë‹¤ìŒ ì§„í–‰ì„ ì„ íƒí•˜ì„¸ìš”",
                gr.update(value="", visible=False),
                gr.update(visible=False),
            )

        followup_send.click(
            on_send_followup,
            inputs=[followup_tb, ollama_url, model_dd, state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status, followup_tb, followup_send],
            queue=True,
        )
        opt_5.click(
            proceed,
            inputs=[gr.State("opt5"), user_question, system_rules, ollama_url, model_dd, state],
            outputs=[chat, start_btn, stop_btn, opt_1, opt_2, opt_3, opt_4, opt_5, status],
            queue=True,
        )

    return demo


if __name__ == "__main__":
    app = main()
    # Use queue with defaults for compatibility across Gradio versions
    app.queue().launch()
